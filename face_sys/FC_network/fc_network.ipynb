{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from torch import nn,optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import visdom\n",
    "\n",
    "input_dim = 10\n",
    "n_hidden_1 = 32\n",
    "n_hidden_2 = 16\n",
    "n_hidden_21 = 8\n",
    "n_hidden_22 = 8\n",
    "n_hidden_3 = 8\n",
    "n_hidden_31 = 4\n",
    "n_hidden_32 = 4\n",
    "n_hidden_4 = 16\n",
    "output_dim = 2\n",
    "output_dim1 = 1\n",
    "output_dim2 = 1\n",
    "\n",
    "vis = visdom.Visdom(env = u'face_angle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#网络model\n",
    "class Batch_Net(nn.Module):\n",
    "    def __init__(self,input_dim,n_hidden_1,n_hidden_21,n_hidden_22,n_hidden_31,n_hidden_32,output_dim1,output_dim2):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "                                    nn.Linear(input_dim,n_hidden_1),\n",
    "                                    nn.BatchNorm1d(n_hidden_1),\n",
    "                                    nn.ReLU(True)\n",
    "                                    )\n",
    "        \n",
    "        \n",
    "        self.layer21 = nn.Sequential(\n",
    "                                    nn.Linear(n_hidden_1,n_hidden_21),\n",
    "                                    nn.BatchNorm1d(n_hidden_21),\n",
    "                                    nn.ReLU(True)\n",
    "                                    )\n",
    "        self.layer22 = nn.Sequential(\n",
    "                                    nn.Linear(n_hidden_1,n_hidden_22),\n",
    "                                    nn.BatchNorm1d(n_hidden_22),\n",
    "                                    nn.ReLU(True)\n",
    "                                    )\n",
    "        \n",
    "        self.layer31 = nn.Sequential(\n",
    "                                    nn.Linear(n_hidden_21,n_hidden_31),\n",
    "                                    nn.BatchNorm1d(n_hidden_31),\n",
    "                                    nn.ReLU(True)\n",
    "                                    )\n",
    "        self.layer32 = nn.Sequential(\n",
    "                                    nn.Linear(n_hidden_22,n_hidden_32),\n",
    "                                    nn.BatchNorm1d(n_hidden_32),\n",
    "                                    nn.ReLU(True)\n",
    "                                    )\n",
    "        self.layer_out1 = nn.Sequential(\n",
    "                                    nn.Linear(n_hidden_31,output_dim1),\n",
    "                                    )\n",
    "        self.layer_out2 = nn.Sequential(\n",
    "                                    nn.Linear(n_hidden_32,output_dim2),\n",
    "                                    )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.layer1(x)\n",
    "        x21 = self.layer21(x)\n",
    "        x22 = self.layer22(x)\n",
    "        x31 = self.layer31(x21)\n",
    "        x32 = self.layer32(x22)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #x = self.layer4(x)      #想去掉某层，在这里就足够了。\n",
    "        '''\n",
    "        x1 = self.layer_out1(x)\n",
    "        x2 = self.layer_out2(x)\n",
    "        x = t.cat((x1,x2),1)           #这3行，即使和之前output_dim = 2效果是一样的，但是，说明可以使用cat。哈哈哈哈！\n",
    "        '''\n",
    "        x_out1 = self.layer_out1(x31)\n",
    "        x_out2 = self.layer_out2(x32)\n",
    "        x = t.cat((x_out1,x_out2),1)\n",
    "        \n",
    "        #print(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#网络model\n",
    "class Batch_Net1(nn.Module):\n",
    "    def __init__(self,input_dim,n_hidden_1,n_hidden_2,n_hidden_3,output_dim):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "                                    nn.Linear(input_dim,n_hidden_1),\n",
    "                                    nn.BatchNorm1d(n_hidden_1),\n",
    "                                    nn.ReLU(True)\n",
    "                                    )\n",
    "        self.layer2 = nn.Sequential(\n",
    "                                    nn.Linear(n_hidden_1,n_hidden_2),\n",
    "                                    nn.BatchNorm1d(n_hidden_2),\n",
    "                                    nn.ReLU(True)\n",
    "                                    )\n",
    "        self.layer3 = nn.Sequential(\n",
    "                                    nn.Linear(n_hidden_2,n_hidden_3),\n",
    "                                    nn.BatchNorm1d(n_hidden_3),\n",
    "                                    nn.ReLU(True)\n",
    "                                    )\n",
    "        self.layer_out = nn.Sequential(\n",
    "                                    nn.Linear(n_hidden_3,output_dim),\n",
    "                                    #nn.BatchNorm1d(output_dim),\n",
    "                                    #nn.ReLU(True)\n",
    "                                       )\n",
    "        #self.softmax = nn.Softmax()\n",
    " \n",
    "        \n",
    "        #x = self.layer4(x)      #想去掉某层，在这里就足够了。\n",
    "        '''\n",
    "        x1 = self.layer_out1(x)\n",
    "        x2 = self.layer_out2(x)\n",
    "        x = t.cat((x1,x2),1)           #这3行，即使和之前output_dim = 2效果是一样的，但是，说明可以使用cat。哈哈哈哈！\n",
    "      \n",
    "        x_out1 = self.layer_out1(x31)\n",
    "        x_out2 = self.layer_out2(x32)\n",
    "        x = t.cat((x_out1,x_out2),1)\n",
    "        '''\n",
    "    def forward(self,x):\n",
    "        #print(\"x.shape\",x.shape)\n",
    "        x = self.layer1(x)\n",
    "        #print(\"x.shape1\",x.shape)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer_out(x)\n",
    "        #x = self.softmax(x)\n",
    "        #print(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset类\n",
    "from torch.utils import data\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Feature_Points(data.Dataset):\n",
    "    def __init__(self,root):\n",
    "        all_folds = os.listdir(root)\n",
    "        folds = [os.path.join(root,img) for img in all_folds]\n",
    "        self.all_files = []\n",
    "        for path in folds:\n",
    "            one_files = os.listdir(path)\n",
    "            files = [os.path.join(path,img) for img in one_files]\n",
    "            self.all_files.extend(files) \n",
    "         \n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        txt_path = self.all_files[index]\n",
    "        label = 1 if 'front_face' in txt_path.split(\"\\\\\")[-2] else 0\n",
    "        fo = open(txt_path)\n",
    "        data_str = fo.read()\n",
    "        array = np.array(data_str.split(\",\")).astype(float)\n",
    "        fo.close()\n",
    "        data = t.from_numpy(array)\n",
    "        \n",
    "        return data, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.all_files)        \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "learning_rate = 1e-4\n",
    "num_epoches = 800\n",
    "weight_decay = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       "  0.4612  0.4487  0.8223  0.5256  0.6335  0.7366  0.2834  0.7729  0.5736  0.8326\n",
       "  0.2757  0.3660  0.7407  0.3447  0.4925  0.6085  0.3409  0.7983  0.7258  0.7779\n",
       "  0.3154  0.3644  0.7716  0.3448  0.5968  0.5794  0.3677  0.7608  0.7734  0.7483\n",
       "  0.3020  0.3954  0.7400  0.4733  0.4417  0.5995  0.2189  0.7325  0.6008  0.7954\n",
       "  0.6492  0.4091  0.8906  0.4500  0.8521  0.7079  0.4967  0.7656  0.7439  0.8241\n",
       "  0.3272  0.3831  0.8061  0.3215  0.5980  0.6500  0.3644  0.7529  0.8145  0.7021\n",
       "  0.5609  0.3310  0.8972  0.3852  0.7833  0.5982  0.3953  0.7613  0.6212  0.8020\n",
       "  0.2153  0.3471  0.6643  0.3439  0.3688  0.5385  0.2619  0.7663  0.6070  0.7629\n",
       "  0.2923  0.3972  0.7752  0.4222  0.4887  0.6263  0.2753  0.7673  0.7036  0.7842\n",
       "  0.3414  0.4072  0.7887  0.3970  0.6202  0.6090  0.4265  0.7886  0.7428  0.7908\n",
       "  0.1614  0.3766  0.5068  0.3869  0.1538  0.5960  0.1498  0.7886  0.4369  0.7875\n",
       "  0.4440  0.3656  0.8876  0.3904  0.7282  0.6389  0.4161  0.7518  0.7791  0.7728\n",
       "  0.1866  0.3707  0.3984  0.3642  0.0410  0.5383  0.1476  0.7490  0.2957  0.7395\n",
       "  0.2420  0.4095  0.7027  0.4089  0.4577  0.6171  0.3265  0.7895  0.6540  0.7918\n",
       "  0.3064  0.3522  0.7974  0.3846  0.5230  0.5779  0.3059  0.7456  0.7052  0.7821\n",
       "  0.3091  0.3728  0.7878  0.3912  0.5413  0.6957  0.3369  0.8218  0.6984  0.8298\n",
       "  0.3733  0.3759  0.8315  0.3556  0.6625  0.5879  0.4422  0.7990  0.8081  0.7876\n",
       "  0.3233  0.3894  0.7930  0.3577  0.5970  0.6057  0.3656  0.7808  0.7904  0.7473\n",
       "  0.5815  0.3473  0.9639  0.3729  0.9157  0.6120  0.5798  0.7932  0.9025  0.8092\n",
       "  0.1616  0.4428  0.5628  0.3545  0.3113  0.6314  0.3068  0.8327  0.6389  0.7702\n",
       "  0.1930  0.3873  0.6289  0.3493  0.3634  0.5889  0.2759  0.7958  0.6379  0.7672\n",
       "  0.1714  0.2971  0.4295  0.2665  0.1985  0.4910  0.2847  0.7559  0.4468  0.7390\n",
       "  0.2155  0.3840  0.6788  0.4377  0.3597  0.5944  0.1892  0.7448  0.5759  0.7802\n",
       "  0.1647  0.5280  0.4730  0.4274  0.2375  0.6940  0.3511  0.8932  0.6152  0.8085\n",
       "  0.4254  0.4186  0.8739  0.4112  0.7220  0.6225  0.4853  0.7981  0.7980  0.7918\n",
       "  0.5280  0.3652  0.9426  0.3752  0.8117  0.6213  0.5288  0.7892  0.8275  0.7895\n",
       "  0.6828  0.3631  0.9497  0.4069  0.9819  0.5838  0.6316  0.7638  0.8527  0.7989\n",
       "  0.2710  0.3901  0.7012  0.3495  0.4974  0.5279  0.3751  0.7735  0.7035  0.7455\n",
       "  0.6540  0.4140  0.8965  0.3803  1.0360  0.5585  0.7366  0.7860  0.9420  0.7565\n",
       "  0.1775  0.3831  0.6096  0.3815  0.2641  0.6039  0.2332  0.7952  0.5453  0.7941\n",
       "  0.5420  0.3555  0.9086  0.3480  0.8311  0.5862  0.5162  0.7821  0.8261  0.7706\n",
       "  0.2742  0.4298  0.6717  0.2829  0.5563  0.5615  0.5053  0.8152  0.8087  0.7031\n",
       "  0.1543  0.3851  0.3703  0.3924  0.0264  0.5680  0.2028  0.7713  0.3780  0.7670\n",
       "  0.2032  0.3333  0.6563  0.3286  0.3415  0.5134  0.2447  0.7551  0.5949  0.7480\n",
       "  0.3244  0.3372  0.7784  0.3565  0.5549  0.5105  0.3554  0.7439  0.6886  0.7642\n",
       "  0.4658  0.3697  0.8973  0.3497  0.7766  0.6073  0.5127  0.8016  0.8331  0.7782\n",
       "  0.1958  0.3518  0.5866  0.3132  0.3433  0.5898  0.3362  0.7809  0.6158  0.7557\n",
       "  0.2979  0.3878  0.7748  0.3776  0.5532  0.6804  0.3519  0.8021  0.7225  0.7876\n",
       "  0.2453  0.3671  0.6894  0.3377  0.4568  0.5928  0.3630  0.7931  0.6899  0.7771\n",
       "  0.2135  0.4258  0.6225  0.2664  0.4829  0.5791  0.4540  0.8273  0.7690  0.7098\n",
       "  0.2651  0.4025  0.7016  0.3566  0.4778  0.5816  0.3603  0.7939  0.6978  0.7657\n",
       "  0.2796  0.3744  0.7195  0.3698  0.4732  0.6057  0.3141  0.7665  0.6889  0.7544\n",
       "  0.3555  0.3714  0.8191  0.3689  0.6059  0.6230  0.3907  0.7885  0.7388  0.7788\n",
       "  0.3447  0.4054  0.8016  0.3574  0.6580  0.5773  0.4693  0.7952  0.8098  0.7584\n",
       "  0.7240  0.2988  1.0432  0.3027  1.0203  0.5057  0.6766  0.7527  0.8824  0.7614\n",
       "  0.3167  0.3565  0.7812  0.3692  0.5413  0.5974  0.3491  0.7825  0.7058  0.7934\n",
       "  0.2046  0.2766  0.6504  0.3283  0.3631  0.5667  0.1969  0.7306  0.5821  0.7665\n",
       "  0.4722  0.4027  0.8649  0.3260  0.8247  0.5734  0.6168  0.8083  0.9186  0.7378\n",
       "  0.2847  0.4356  0.7438  0.3899  0.5378  0.6574  0.4020  0.8143  0.7496  0.7717\n",
       "  0.3460  0.4182  0.7529  0.3029  0.7047  0.5332  0.5703  0.7977  0.8800  0.6958\n",
       "  0.3161  0.3688  0.7745  0.4285  0.5017  0.6101  0.2873  0.7564  0.6309  0.8047\n",
       "  0.5122  0.3665  0.9061  0.4345  0.7460  0.6273  0.4345  0.7552  0.7411  0.8103\n",
       "  0.4390  0.3751  0.8756  0.3668  0.7200  0.6327  0.4696  0.7967  0.8043  0.7894\n",
       "  0.2261  0.4249  0.6996  0.3490  0.4821  0.5831  0.3858  0.7789  0.7561  0.7252\n",
       "  0.1670  0.3971  0.5859  0.3602  0.2772  0.6054  0.2814  0.8008  0.6029  0.7708\n",
       "  0.6824  0.4034  0.9592  0.4310  0.9828  0.6432  0.5981  0.8138  0.8053  0.8383\n",
       "  0.4541  0.4025  0.8510  0.3798  0.7458  0.6025  0.5004  0.7863  0.7801  0.7673\n",
       "  0.1482  0.3526  0.4085  0.3450  0.0743  0.5667  0.1879  0.7824  0.3920  0.7786\n",
       "  0.2284  0.4233  0.6594  0.3524  0.4354  0.6540  0.3682  0.8187  0.7075  0.7650\n",
       "  0.1198  0.3087  0.3804  0.2804  0.0483  0.5593  0.2650  0.7852  0.4410  0.7630\n",
       "  0.5118  0.3491  0.9232  0.3006  0.8719  0.5685  0.5564  0.7722  0.8748  0.7408\n",
       "  0.1215  0.3201  0.4295  0.3234  0.0933  0.5640  0.2061  0.7786  0.4270  0.7847\n",
       "  0.5692  0.3642  0.9191  0.3137  0.9801  0.5062  0.6912  0.7837  0.9601  0.7406\n",
       "  0.2668  0.4002  0.7148  0.3720  0.4780  0.5887  0.3899  0.8031  0.6915  0.7894\n",
       " [torch.DoubleTensor of size 64x10], \n",
       "  0\n",
       "  1\n",
       "  1\n",
       "  1\n",
       "  0\n",
       "  1\n",
       "  0\n",
       "  1\n",
       "  1\n",
       "  1\n",
       "  0\n",
       "  1\n",
       "  0\n",
       "  1\n",
       "  1\n",
       "  1\n",
       "  1\n",
       "  1\n",
       "  0\n",
       "  1\n",
       "  1\n",
       "  0\n",
       "  1\n",
       "  0\n",
       "  1\n",
       "  1\n",
       "  0\n",
       "  1\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  1\n",
       "  0\n",
       "  1\n",
       "  1\n",
       "  1\n",
       "  1\n",
       "  1\n",
       "  1\n",
       "  1\n",
       "  1\n",
       "  1\n",
       "  1\n",
       "  1\n",
       "  0\n",
       "  1\n",
       "  1\n",
       "  1\n",
       "  1\n",
       "  1\n",
       "  1\n",
       "  1\n",
       "  1\n",
       "  1\n",
       "  1\n",
       "  0\n",
       "  1\n",
       "  0\n",
       "  1\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  1\n",
       " [torch.LongTensor of size 64])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Feature_Points(\"D:\\\\newfolder7\")\n",
    "dataloader = DataLoader(dataset,batch_size=batch_size,shuffle=True)\n",
    "#data, label = dataset[5]\n",
    "#data,label\n",
    "#'''\n",
    "dataiter = iter(dataloader)\n",
    "datas,labels = next(dataiter)\n",
    "datas,labels\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Batch_Net(input_dim,n_hidden_1,n_hidden_21,n_hidden_22,n_hidden_31,n_hidden_32,output_dim1,output_dim2)\n",
    "model = Batch_Net1(input_dim,n_hidden_1,n_hidden_2,n_hidden_3,output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if t.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion.cuda()   #P161\n",
    "optimizer = optim.Adam(model.parameters(),lr=learning_rate,weight_decay=weight_decay)  \n",
    "\n",
    "#loss_meter = meter.AverageValueMeter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#参考的siamese网络的那个结构\n",
    "sum_loss = t.Tensor([0])     #多一个0元素，不会影响平均值\n",
    "for epoch in range(0,num_epoches):\n",
    "    for i,data in enumerate(dataloader,0):   #表示i 从0开始\n",
    "        data_batch, label = data\n",
    "        data_batch, label = Variable(data_batch.float()).cuda(),Variable(label).cuda()\n",
    "        output = model(data_batch)\n",
    "        #print(output.cpu())\n",
    "        #print(output)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = criterion(output,label)\n",
    "        loss.backward()            #我终于知道，我的图为什么不下降了，我忘记反向传播了。。。。。。。。。。。。\n",
    "        optimizer.step()\n",
    "        #sum_loss.append(loss)\n",
    "        sum_loss = t.cat((sum_loss,loss.cpu().data),0)\n",
    "        x = i+epoch*len(dataloader)      #此时的x还只是个常数，不是列表，也不是np.array()\n",
    "        \n",
    "        if x%50 == 49:\n",
    "            loss_aver = sum_loss.mean()\n",
    "            sum_loss = t.Tensor([0])\n",
    "            #vis.line(X=x, Y=loss.cpu().data, win='face_angle',opts={'title': 'loss-batch', 'xlabel':'x','ylabel':'y'},update='append' if i>=0 else None)\n",
    "            vis.line(X=t.Tensor([x]), Y=t.Tensor([loss_aver]), win='face_angle1',opts={'title': 'loss-batch1', 'xlabel':'batch_num','ylabel':'loss_aver'},update='append' if x>49 else None)\n",
    "            #vis.updateTrace(X=t.Tensor([x]),Y=t.Tensor([loss_aver]),win='face_angle', name='7')\n",
    "t.save(model.state_dict(),\"C:\\\\Users\\\\SiChen\\\\face_sys\\\\FC_network\\\\model_save\\\\fc_network_800_32_16_NoSoftmax_lr1e-5.pth\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#实验，t.cat在这里有没有用\n",
    "sum_loss = t.cat((sum_loss,loss.cpu().data),0)\n",
    "#sum_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "2\n",
      "[ True False  True]\n",
      "2\n",
      "Variable containing:\n",
      " 2\n",
      "[torch.ByteTensor of size 1]\n",
      "\n",
      "c1 Variable containing:\n",
      " 1\n",
      " 0\n",
      " 1\n",
      "[torch.ByteTensor of size 3]\n",
      "\n",
      "\n",
      " 1\n",
      " 0\n",
      " 1\n",
      "[torch.ByteTensor of size 3]\n",
      "\n",
      "c1.data[0] 1\n",
      "Variable containing:\n",
      " 1\n",
      "[torch.ByteTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 0\n",
      "[torch.ByteTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 1\n",
      "[torch.ByteTensor of size 1]\n",
      "\n",
      "before process c1[c1>0]    Variable containing:\n",
      " 1\n",
      " 1\n",
      "[torch.ByteTensor of size 2]\n",
      "\n",
      "t1 \n",
      " 0\n",
      " 1\n",
      " 0\n",
      "[torch.ByteTensor of size 3]\n",
      "\n",
      "t2 \n",
      " 1\n",
      " 0\n",
      " 1\n",
      "[torch.ByteTensor of size 3]\n",
      "\n",
      "1\n",
      "2\n",
      "c1.data[t1] \n",
      " 0\n",
      "[torch.ByteTensor of size 1]\n",
      "\n",
      "c1.data[t2] \n",
      " 1\n",
      " 1\n",
      "[torch.ByteTensor of size 2]\n",
      "\n",
      "after process c1[c1>0]    Variable containing:\n",
      " 1\n",
      "[torch.ByteTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#通过本例，可以看出list array tensor三者的异同\n",
    "#所以可以对tensor、numpy.array使用sum()函数。但是对list却不可以\n",
    "#在test的统计中，使用的会比较多。\n",
    "\n",
    "a = [1,0,1]\n",
    "b = [1,1,1]\n",
    "c = (a==b)\n",
    "print(c)\n",
    "\n",
    "a0 = np.array(a)\n",
    "b0 = np.array(b)\n",
    "c0 = (a0==b0)\n",
    "print(c0.sum())  #c0.sum()是numpy.int32\n",
    "print(c0)\n",
    "\n",
    "aa = t.Tensor(a)\n",
    "bb = t.Tensor(b)\n",
    "cc = (aa==bb)\n",
    "print(cc.sum())   #cc.sum()是int类型\n",
    "\n",
    "a1 = Variable(aa)\n",
    "b1 = Variable(bb)\n",
    "c1 = (a1==b1)\n",
    "\n",
    "print(c1.sum())   #c1.sum()是torch.ByteTensor。size为1\n",
    "\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!这几个类型，这次可以非常清楚它们的差别了。知道在jupyter中如何区分了!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "print(\"c1\",c1)     #类型为：Variable containing: [torch.ByteTensor of size 3]       是Variable.\n",
    "print(c1.data)     #类型为：[torch.ByteTensor of size 3]  不含Variable containing:。即，不是Variable。只是单纯的Tensor\n",
    "print(\"c1.data[0]\",c1.data[0])  #类型为int\n",
    "\n",
    "\n",
    "for i in c1:\n",
    "    print(i)\n",
    "v= Variable(t.Tensor([1])).type(t.ByteTensor)\n",
    "c1[c1==0]\n",
    "c1[1]\n",
    "\n",
    "a0[1] = 5\n",
    "a0[1]\n",
    "#c1[c1==0]\n",
    "\n",
    "#c1.data[c1.data==0] = t.Tensor([1]).type(t.ByteTensor)      #Variable不知道为什么不可以修改c1[1]，但是numpy.array，list，Tensor都可以。所以通过这种方式。\n",
    "#c1.data[c1.data>0] = t.zeros(2).type(t.ByteTensor)\n",
    "print(\"before process c1[c1>0]   \",c1[c1>0])\n",
    "##============================================================================###\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!通过这段代码，可以将所有的0变成1，所有的1变成0。非常好用啊！！！！！！！！！！！！！！！！！！！！！\n",
    "#此处的c1是Variable.\n",
    "t1 = (c1.data==0)\n",
    "t2 = (c1.data>0)\n",
    "print(\"t1\",t1)\n",
    "print(\"t2\",t2)\n",
    "print(t1.sum())\n",
    "print(t2.sum())\n",
    "print(\"c1.data[t1]\",c1.data[t1])\n",
    "print(\"c1.data[t2]\",c1.data[t2])\n",
    "c1.data[t1] = t.ones(t1.sum()).type(t.ByteTensor)         \n",
    "c1.data[t2] = t.zeros(t2.sum()).type(t.ByteTensor)\n",
    "print(\"after process c1[c1>0]   \",c1[c1>0])\n",
    "###=============================================================================###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 保存模型与加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.save(model.state_dict(),\"C:\\\\Users\\\\SiChen\\\\face_sys\\\\FC_network\\\\model_save\\\\fc_network_800_32_16_NoSoftmax_lr1e-4.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载模型必须在，前面定义模型之后，即在前面的model = ....之后才能执行这一行。\n",
    "model.load_state_dict(t.load(\"C:\\\\Users\\\\SiChen\\\\face_sys\\\\FC_network\\\\model_save\\\\fc_network_800_32_16_softmax.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 以下是测试部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = Feature_Points(\"D:\\\\newfolder17\")\n",
    "test_dataloader = DataLoader(test_dataset,batch_size=3,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 10])\n",
      "torch.Size([3, 10])\n",
      "torch.Size([3, 10])\n",
      "torch.Size([3, 10])\n",
      "torch.Size([3, 10])\n",
      "torch.Size([3, 10])\n",
      "torch.Size([3, 10])\n",
      "torch.Size([3, 10])\n",
      "torch.Size([3, 10])\n",
      "torch.Size([3, 10])\n",
      "torch.Size([3, 10])\n",
      "torch.Size([3, 10])\n",
      "torch.Size([3, 10])\n",
      "torch.Size([3, 10])\n",
      "torch.Size([3, 10])\n",
      "torch.Size([3, 10])\n",
      "torch.Size([3, 10])\n",
      "torch.Size([3, 10])\n",
      "torch.Size([3, 10])\n",
      "torch.Size([3, 10])\n",
      "torch.Size([3, 10])\n",
      "torch.Size([3, 10])\n",
      "torch.Size([3, 10])\n",
      "torch.Size([3, 10])\n",
      "torch.Size([3, 10])\n",
      "torch.Size([3, 10])\n",
      "torch.Size([3, 10])\n",
      "torch.Size([3, 10])\n",
      "torch.Size([3, 10])\n",
      "torch.Size([3, 10])\n",
      "torch.Size([3, 10])\n",
      "torch.Size([3, 10])\n",
      "torch.Size([3, 10])\n",
      "torch.Size([3, 10])\n",
      "torch.Size([3, 10])\n",
      "Variable containing:\n",
      " 103\n",
      "[torch.ByteTensor of size 1]\n",
      "\n",
      "105\n",
      "0.9809523809523809\n"
     ]
    }
   ],
   "source": [
    "model.cuda()\n",
    "model.eval()\n",
    "test_right_num = 0\n",
    "for data in test_dataloader:\n",
    "    data_batch,label = data\n",
    "    print(data_batch.shape)\n",
    "    data_batch,label = Variable(data_batch.float(),volatile=True).cuda(),Variable(label,volatile=True).cuda()\n",
    "    out = model(data_batch)\n",
    "    \n",
    "    _,pred = t.max(out.cpu(),1)\n",
    "    num_correct = (pred==label.cpu()).sum()\n",
    "    test_right_num += num_correct\n",
    "    \n",
    "    qq = (pred==label.cpu())\n",
    "    t1 = (qq.data==0)\n",
    "    t2 = (qq.data>0)\n",
    "    qq.data[t1] = t.ones(t1.sum()).type(t.ByteTensor)         \n",
    "    qq.data[t2] = t.zeros(t2.sum()).type(t.ByteTensor)\n",
    "    #print(\"after process c1[c1>0]   \",t.Tensor(data)([qq>0]))#????????????????????????\n",
    "    \n",
    "acc = (float(test_right_num))/len(test_dataset)\n",
    "print(test_right_num)\n",
    "print(len(test_dataset))\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3754\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1476"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(os.listdir(\"D:\\\\newfolder7\\\\front_face\")))\n",
    "(len(os.listdir(\"D:\\\\newfolder7\\\\side_face\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(os.listdir(\"D:\\\\newfolder17\\\\front_face\")))\n",
    "(len(os.listdir(\"D:\\\\newfolder17\\\\side_face\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.888888888888889\n",
      "2.6666666666666665\n",
      "2.6666666666666665\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.913580246913581"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
    "b = np.array([[9,8,7],[6,5,4],[3,2,1]])\n",
    "c = np.maximum(a,b)\n",
    "a1 = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
    "b1 = np.array([[9,8,7],[6,5,4],[3,2,1]])\n",
    "c1 = np.minimum(a,b)\n",
    "d = c - c1\n",
    "print(d[1].var())\n",
    "print(d[2].var())\n",
    "print(d[0].var())\n",
    "(d[0].var()+d[1].var()+d[2].var())/3\n",
    "d.var()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
